# Terrascale: Planet-Scale Analytics

Terrascale is a cloud-native data storage, analysis and visualization engine with first-class support for geospatial data and real-time streaming. It includes

* An HTAP database with ANSI SQL support and Python programmability
* First-class support for geospatial vector data and raster images
* First-class support for incremental queries over real-time data
* A programmable, embeddable visualizer for scalar, vector-image and raster-image queries
* A collaborative editor with real-time, CRDT-resolved edits and version history
* Source data snapshots and version control

Terrascale is designed to support a variety of user scenarios involving real-world data and measurements, including online operations and offline analysis, all on a single dataset. For example, a customer might use Terrascale to build realtime monitoring, visualization and alerting for autonomous vehicles, then run large-scale analysis to mine their operational data for trend insights. Others might use Terrascale to integrate satellite raster data with on-the-ground sensors data collected in real time via an IoT network, to feed online or offline analysis and/or monitoring. Terrascale is well suited for geographic analysis scenarios, but it is not a full GIS suite; however, its extensibility model allows it to act as a potential basis for one.

Terrascale consists of the following software components:

* TerrascaleDB, a cloud-native distributed storage and query engine
* The Tile Delivery Network (TDN), a CDN for raster image tiles
* Terrascale's Visualizer, a high-performance in-browser 2D rendering engine and database client
* Terrascale Projects, a real-time collaborative, version-controlled studio for scientific analysis
* The Terrascale API, which provides frontend and backend programability for the above
* Terrascale, an in-browser webapp which provides a unified experience for all the above

Although the initial build of Terrascale will be optimized for cloud computing scenarios, we plan to design up front to make it feasible to deploy Terrascale on the edge, as (for example) a ruggedized box housing a small cluster of compute/storage nodes running Terrascale. We abstract away cloud-native storage systems and compute orchestration APIs to make it simpler to build a local clustering system, and we have designed Terrascale to work with a very small number of compute nodes from the start. Fully lighting up this scenario would require additional currently-off-roadmap work, such as a synchronization engine for moving portions of datasets between the edge and the cloud, as well as implementations of our storage-layer abstractions suitable for use on bare-metal clusters.

In this document, we will build Terrascale from the bottom up, starting from our cloud-native data management and tile rendering architecture, then moving onto client-side experiences such as the visualization engine and extensibility surface, and finally onto the Projects experience hosted on Terrascale's website.

## Cloud Provisioning

Terrascale is a multi-tenant service through which customers manage single-tenant TerrascaleDB instances running in a third-party cloud such as AWS, Azure or GCP. In this way, TerrascaleDB clusters are similar to Snowflake's virtual warehouse concept. This gives customers fine-grained control over the compute-vs-cost tradeoff, and isolates resource-intensive analytical queries to the customer's private compute cluster. This also provides strong isolation of customer data for security-sensitive scenarios than a multi-tenant compute cloud could provide.

TODO our brain needs to understand VM geometry so we can programmatically deploy clusters where each VM has access to the right durable disk device.

## Database Workloads

TerrascaleDB is a hybrid transactional/analytical database with support for high-volume short-lived transactional queries in tandem with low-volume resource-intensive analytical (cross-tabulation) queries. Queries can execute on a finite point in time, or update continually over real-time streaming data. Users can independently scale compute and storage resources as a means to control the cost-performance tradeoff of their queries.

Kinds of queries we expect and optimize for include...

**Point reads and writes**: Workloads where users build their datasets from operational data such as sensor data ingestion or logging human interactions with software, will be characterized by point writes, potentially at a high rate with high parallelism, with each write inserting a single row. From Terrascale's point of view, these workloads may be write-only (when users host their operations in a separate system and stream updates to Terrascale, e.g. using a mechanism like Postgres's change data capture), or they may be a read-write-mix if users directly host their operations in Terrascale. In general, the point reads and writes generated by this aspect of a customer workload will each be confined to a single table partition.

**Time- and space-windowed queries**: As data streams into a Terrascale instance, we expect to see queries over small space and time ranges as users export their data for human visualization or programmatic observability (monitoring and alerts). We usually expect these small windowing queries to run periodically at a low rate, but anticipate that sometimes these views may be "slashdotted" and thus see a high rate of read-only traffic. These queries may run finitely or use incremental streaming, and each query will usually cross only a small number of table partitions.

**Bulk import**: Customers using Terrascale for offline analytics may bulk-import data from other sources, either as a one-time event to populate a cluster or as part of a nightly ETL database dump. This kind of workload is characterized by just one or two write transactions each inserting many, many rows, crossing all partitions of a cluster. It is unusual for customers to run other kinds of queries while this is ongoing; for example, an ETL customer will likely load all data into Terrascale before starting any new analytical queries. As such, it is okay for a bulk import to cause a storm of traffic that doesn't settle until the import has completed. Bulk import operations need to be restartable, whether through a checkpoint/restart protocol or idempotency.

**Analysis**: Users are expected to use their datasets, whether built operationally or via obtained via a bulk import, for large cross-tabulating analysis queries featuring complex filters, grouping, aggregations, cross-table joins, and spatial joins. Although users are expected to run few of theses kinds of queries at a time, each may necessitate a large amount of compute, storage and network traffic. Additionally, analytical queries may either run in a finite point-in-time or as incremental realtime streaming; the latter is useful for scenarios like anomaly detection and alerting.

**Complex updates**: As part of cleaning and maintaining large and complex datasets, we expect users will occasionally need to issue one-off 'corrections' to their datasets. These are characterized by large, complex transactions which may read many rows, do some sort of computation, and push many updates. Data cleaning transactions and table schema changes are examples of this kind of update. These large transactions are challenging to handle well, since they may cross many table partitions and need to have full serializable semantics; however, because they are low volume and only happen occassionally, we don't optimize for these. We simply ensure these produce the correct results without causing excessive downtime for real-time and operational workloads.

**Bulk export**: Customers may wish to do large, point-in-time bulk reads to export data from Terrascale as part of a wider ETL process that copies or backs up Terrascale data to a separate system. Like bulk import, exports need to provide consistency and restartability, whether the latter is achieved through idempotency or a checkpoint/restart protocol.

TODO the data types are expected to be scalar, 1D spatial, multidimensional spatial, time; this is not a document store and there is no current plan to handle semi-structured data.

## Storage Model

TerrascaleDB includes a thin storage abstraction layer responsible for interfacing with external storage APIs and network interfaces. This eases the process of moving Terrascale between cloud providers and potentially makes it simpler to light up custom hardware solutions and on-prem scenarios. 

The storage layer defines three kinds of storage:

* **Block devices** provide a high-cost, low-latency file system with write-in-place semantics. Some block devices are **durable**, making them appropriate for staging customer data. Others are **volatile**, making them appropriate for cache-spilling, staging external writes and storing intermediate results. Block devices are strongly affinitized to a single node in the cluster.
* **Object stores** provide low-cost, high latency buckets of write-once unstructured data objects. Depending on the implementation, object stores may be strongly consistent or eventually consistent; as such, the object store abstraction is always exposed as eventually consistent. Object stores are not affinitized to nodes, and are often used for cross-node sharing.
* **Catalogs** are small, strongly consistent key-value stores. These can be used for bookkeeping information needed to detect and correct for object store eventual consistency artifacts, and are also appropriate for other bookkeeping, like the set of tables which exist and their schemas.

The indexing layer consumes this storage model for storing TerrascaleDB's row data and log-structured merge index. Client writes are staged in a write-ahead log stored on a durable block device, and runs of log-structured merge trees are staged to volatile block devices during the checkpointing and merge processes. These structures are lazily destaged into object-store objects and read back in through a tiered memory / volatile block store cache. One catalog store per database instance is used to track tables, schemas, durable block files of note (e.g. the WAL), object store objects (such as the row store and LSM runs), and information needed to recover the WAL (such as replay start pointer).

In the cloud, durable block devices are cloud block stores such as EBS or managed disks accessed through a local file system. Volatile block devices are locally-attached disk instances, also accessed through the local file system. Object stores wrap cloud object stores like S3 and block blobs. Catalogs wrap cloud-native table storage like DynamoDB or Azure's tables. 

In the future, TerrascaleDB may be extended to run on small clusters of hardware nodes (e.g. an on-the-go ruggedized 'edge' offering). In this setup, durable block devices and object stores might be implemented as a custom replicated protection protocol over physical disks, with n-way replication for durable block storage and Reed-Solomon erasure coding for objects. Volatile block devices might be implemented as non-protected disk storage (no replication or erasure coding). Catalogs might be implemented using a third-party / open-source database running on each node in the cluster.

TODO a global URI scheme so e.g. a catalog can point to data across different storage subsystems. Example: a row store segment may exist in durable block storage or an object store; the catalog uses a URI to identify which kind of store the segment is currently in and where it is within that store.

TODO Where do checksums live? Can this layer checksum by itself, or are we relying on the higher level layers to decide where in the file checksums go? It's possible for a GFS-like stream to manage checksums automatically, but we don't have structured append-only storage. And we likely have file system checksumming underneath us too, but we want to checksum as early as possible and pass checksums down the stack as far as we can. I think there are relatively reasonable checksumming strategies for the indexing layer, like per log flush buffer and per-b-tree page, so it's not a disaster if we push checksumming up a level. But if we can make it transparent at this layer, that's nice. One of the problems with transparent checksumming is this layer doesn't know the read block size of the parent.

TODO what about encryption? Do we manually encrypt all of our own stuff in-proc, or do we rely on a file system filter kind of thing to do full-volume encryption transparently? Having this layer transparently encrypt stuff in-proc makes more sense here, you basically need a key store with a KEK or something and then you use your catalog to remember which external secret store key is the one you need for these files. Then you plug in a protected key blob into the block/object layer when opening the block/object store and have the abstraction layer transparently encrypt/decrypt on the fly. Ideally this can be done at the abstraction layer, so that the individual store implementations don't have to manage encryption/decryption logic themselves. Also, it'd be nice to include a new checksum on encrypted data post-encryption, which is another good argument for checksumming at this store layer. But we already talked about why that's not such a great idea. Also, it's worth noting the encrypted blob may have extra metadata too, like a unique pseudorandom IV per block so we're not doing plain virtual code book or whatever it's called.

### Row Stores

TODO here's a quick draft:

* The row store is an unordered collection of append-only segments about 64-256MB in size
* Segments contain raw row value tuples
* Segments may be stored as files in durable block storage
* Segments may be stored as objects in object storage (read in through volatile block caching)
* An active segment is still being built; you can append new rows to one
* Active segments can only be stored on block storage devices
* A sealed segment is done; you cannot modify it ever again
* Sealed segments initially exist in block storage, but are later offloaded to object storage
* Segment metadata is tracked in the database's catalog
* Catalog metadata includes allocated segment IDs, seal state, and URI to underlying files
* Entries in the log are indexed by a 64-bit row identifiers (rowids)
* Rowids are globally unique in the context of a single database table
* A 64-bit rowid is split into a 32-bit table-global segment ID and a 32-bit segment-local entry ID
* The catalog tracks which 32-bit table-global segment IDs are allocated

I need to leave to get Max but

* With the high bits not having an order-related meaning, we're now free to have multiple active segs
* That in turn makes it very feasible to have active GC segs in parallel with active write-ahead segs
* If we wish, we can demote the segid to 16 bits if it helps make the bitmaps smaller

We also need to support opaque metadata alongside each individual row, so we can piggypack write-ahead information in this store and use it for recovery when a table partition is loaded.

Indexing works as follows

* For segments still being built, the index is stored in cache as B+ tree with a build-forward algo
* An in-progress index is not written out; it's rebuilt on recovery when you replay segments
* When you checkpoint a segment to disk, you append the B+ tree and a footer with segment metadata
* In memory, we store a mapping of (low row ID) to (segment number) map managed with RCU
* That map is used to read back segment root pages in through the cache
* The segment root pages are used to find more pages through cache as well as segment data itself

## Indexing Structures

TODO: at some point while I was drafting this, I decided to go with a full dual-indexing route instead of the original plan, which was to use traditional LSM up until we have a checkpoint or two of data worth merging, at which point the amount of data we're handling becomes large enough that it's worth dual-indexing into row and columnar storage. I don't remember how I got off that track and I want to get back on that track.

So basically what I want here is

* There's a row store as we described above (I've moved that content out of this section)
* The primary index is an LSM-tree of (primary key) to (row ID in row store)
* We log directly to the row store for short-term recovery
* We also lazily build an index-only journal with (primary key to row id) mapping, possibly in whatever order is simplest for the transaction isolation (concurrency control) system
* In memory, we have a primary key to row ID BST with multiversioning support
* For any secondary indexes, we also keep a secondary key to row ID BST with multiversioning
* We checkpoint each primary- and secondary-index to LSM checkpoint B+ trees on disk
* Transactional and analytical queries all 
* 

---

TerrascaleDB indexes data using a log-structured merge strategy inspired by ideas from Apache Druid and WiscKey.

Like in WiscKey, new rows are logged to a dedicated 'row store' in arrival order, are indexed externally using log-structured merge trees, and are cleaned up using copy-forward garbage collection. Storing rows out-of-page with respect to the log-structured merge tree index incurs additional read amplification (since rows are not in the tree and must be fetched with an additional read after the tree lookup) but allows for lower write amplification (since the rows themselves do not need to be rewritten with the rest of the tree on checkpoint and merge). This design is a particularly good match for modern flash memory, which generally provides very good latency/throughput on high-queue depth random reads, but is prone to high write latency tails and early wearout for write-heavy workloads.

TerrascaleDB uses a dual indexing strategy over this arrival-order row store. One of the indexes is a traditional row-oriented log-structured merge tree which maps primary keys to the corresponding location in the row store. The other index is a columnar inverted index inspired by Apache Druid. The row index is well-suited for answering OLTP queries that request individual rows or small ranges, and the columnar index is well-suited for OLAP (cross-tabulation) queries that feature large-scale aggregation and grouping with potentially complex multidimensional filters and large joins over many tables.

Dual-indexing enables hybrid transactional/analytical processing, at the cost of roughly doubling the write- and storage-amplification overhead compared to a row-only or column-only index. Storing (relatively large) row payloads out-of-page in a separate, garbage-collected row store as suggested by WiscKey reduces the write- and space-amplification overhead of dual-indexing to what we believe is a manageable level.

TerrascaleDB's indexing layer supports range types, and automatically breaks down overlapping ranges in the index transparently, on-the-fly. The columnar index supports efficient lookup of objects which intersect an n-dimensional bounding box specified by one range along each axis. N-dimensional bounding box queries are used by the query layer to generate candidate result sets, which are further refined using fine-grained collision detection algorithms for spatial queries and spatial joins.

This section begins with an overview of the different on-disk structures that store and index data in a single TerrascaleDB table. We then discuss algorithms for building and garbage-collecting these structures, and for offloading into object storage. Finally, we discuss mainline query paths for scalar data and spatiotemporal bounding boxes for transactional and analytical processing workloads.

### Memory Tables

TODO I think what I want to do here is to keep row store pages referenced by the memory table resident in memory, and then have the memory table be the same index in an in-memory multi-version binary search tree that it is on disk. Multiversioning with columnar indexing may be quite hard because there's no longer a single bitmap; there's one per version, or there's one bitmap with versioned bits, neither of which is space-efficient. Maybe we can get away with a row-only memory table and emulate columnar by full memory table scans? Maybe we build a columnar memory table lazily? Maybe we have a traditional column store memory table, but an inverted index style on disk?

### Row Tables

TODO this is probably the easiest part of the system, each of these is just a dense (pages fully utilized) B+ tree on disk, where the keys in the tree are row primary keys and the values are row IDs of rows in the row store.

TODO it's probably worth pointing out at some point that it's possible for users to ask us to maintain a secondary index, and we do that as easily as creating a primary index. There's an opportunity to elide an entire row index if it's a single column we already indexed, but that's a micro-optimization we can look into later.

### Column Tables

TODO this is an LSM tree where keys are unique column values that appear in the tree, and the values are row ID lists &mdash; can be in-page for just a few results or an external pointer to a dedicated roaring bitmap page if there's more than just a few. Each entry also stores an up-to-date entry count. For differential trees, you also need a way to specify a set of rows that were deleted from the system so they can't be found in older trees. This should be a separate, optional delete list which is again either a sparse list or a full roaring bitmap.

So far it's seeming likely we'll be using 64-bit row IDs, in which case https://github.com/RoaringBitmap/RoaringFormatSpec specifies extensions for 64-bit row numbers.

### Write-Ahead, Checkpoint and Merge

TODO write-ahead commits to a row store page in memory; the transaction then gets blocked on the page's flush event. That flush comes either when the page is full, or after the first transaction has been waiting 'too long.' Timed flushes can lead to a single page being flushed multiple times, e.g. every time a timeout occurs or on the final flush once the page is full. We handle this by rewriting in place. We put the raw row value as the row entry in the store, and we put our own recovery information in the opaque entry.

All on-disk row- and column-trees are LSM, and work via standard inorder checkpointing/merging algorithms. Mention that we assume we're checkpointing to local volatile SSD storage before we offload to object storage. As such, a reasonable merge policy is basically just "min completion rate with deadline."

### Garbage Collection

TODO this is ... hard, as always. The simplest way to do this is naive mark and sweep: walk the live row index to build occupancy statistics (how many bytes of live data divided by how many bytes), pick which object-store objects we want to collect, do another index walk to identify rows that need to be rewritten with a new row ID

### Queries on Scalar Data

TODO this is pretty standard LSM for transactional. For analytical, we'll discuss it below in the aggregate analysis section

### Range Breakdown and N-D Bounding Box Queries

TODO this is obvious for the column store, each unique range after breakdown lists a bitset of objects that fall along this section of the axis, and a bounding-box query is just an intersection on each axis. The only interesting question here is, can a range appear in a primary key? If so, how does the row index handle that?

## Aggregate Analysis

TODO Anything you can do on unsorted columnar data, you can also do on an inverted index. When you come across a column value in an inverted index, you repeat whatever you would have done with your columnar forward index as many times as there are matching bits in the bitset.

More concrete examples: min just gets the lowest matching column value that has nonzero bits, count distinct and sum just look at number of instances of each column value, and mean is just sum divided by count. Some functions like ranks and percentiles need larger scans and may require more work, and/or approximation algorithms if we can find them (think like the quick select median of medians).

## Transactions and Concurrency Control

TODO this is an LSN-ordered publish log for write-only (WO) transactions, snapshot isolation for read-only (RO) transactions at any scale (point, window, analytical), and a predicate locking scheme for read-write (RW) transactions for the complex update scenario. WO transactions are written to one of the active write-ahead logs with a single record that describes and commits the transactions, whereas RW transactions commit incrementally, with the final commit piggybacked on the last incremental write. Recovery replays committed transactions and ignores uncommited ones; since this is an LSM, there is only a log, so there is nothing to undo.

The problem that needs to be solved here is how to rectify an ahead-of-time LSN ordering scheme with long running RW transactions with unpredictable predicate locking. If the predicates can be found ahead of time, we can ensure serializability easily, but if RW transactions can incrementally lock more and more predicates, it's not clear how to let that happen without locking the whole WO publish queue. An RW transaction can

But that's fine, I think we document two schemes. If it's possible to predicate lock for an RW transaction ahead of time, then the RW transaction simply acquires all predicate locks, waits for all preceding RW or WO transaction, then executes on its own isolated snapshot view of the world. If for some reason that is not feasible for some queries, those must lock the entire WO queue, which sucks but at least we can do some background buffering so when RW finally publishes, all WOs that were waiting on it show up instantly as well.

Also, I decided to put off discussing the recovery scheme until here.

## Query Path Selection and Optimization

TODO go read redbook.io - Volcano maybe was the major paper in this space?

## Distributed Queries

TODO this is raft for distributed commit of transactions. A good question is who plans and optimizes the query. It could be a random node in the network, or it could be the raft leader (if that wouldn't be a bottleneck, at least the leader is always up to date, right?) or it could be a multi-tenant cloud orchestration service sitting in front of the cluster (but I'd rather do everything in the cluster if possible, since we want to keep open the possibility of shipping a small cluster of nodes in a box).

What's the partitioning strategy? I assume this is single-coordinator, not quorum-writes per row. Then how do we do splits and merges?

## Geospatial

TODO this is columnar range breakdown to find matching items within bounding boxes, then analytical collision detection and other spatial algorithms to finish joins and whatnot

## Incremental Queries and Real-Time Streaming

TODO this is mostly a division into cases, what kinds of things the language supports and how we update these over time.

## Imperative Language Support

TODO I want this to essentially be a way of executing queries, without having to build/parse SQL strings, and help cast them to the types that users want to run calculations over, like numpy arrays or that nd thing that I remember seeing geo people really excited about

## Tile Delivery Network

TODO is this third party CDN? Nodes serving data from inside your network? Just putting tiles into objects and serving them over cloud URLs? Any processing that has to be done, you can do inside your processing cluster, right?

## Tile Processing

TODO obviously this can't be SQL. Should it be shaders? Who executes the shaders?

## Embeddable, Programmable Visualizer

TODO this is a 2D rendering engine for tile and vector data that runs in a native app using C/Vulkan or on the web with Emscripten/WebGPU. 

## Projects and Version Control

TODO define some kind of project object model, introduce CRDTs for common operations, describe a database scheme for tracking version history

## Terrascale.io

TODO first party hosting for all this stuff. Billing and stuff.
