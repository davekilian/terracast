# Terrascale: Planet-Scale Analytics

Terrascale is a cloud-native data storage, analysis and visualization engine with first-class support for geospatial data and real-time streaming. It includes

* An HTAP database with ANSI SQL support and Python programmability
* First-class support for geospatial vector data and raster images
* First-class support for incremental queries over real-time data
* A programmable, embeddable visualizer for scalar, vector-image and raster-image queries
* A collaborative editor with real-time, CRDT-resolved edits and version history
* Source data snapshots and version control

Terrascale is designed to support a variety of user scenarios involving real-world data and measurements, including online operations and offline analysis, all on a single dataset. For example, a customer might use Terrascale to build realtime monitoring, visualization and alerting for autonomous vehicles, then analyze their operational data for trend insights. Others might use Terrascale to integrate satellite raster data with on-the-ground sensors data collected in real time via an IoT network, to feed online or offline analysis and monitoring. Terrascale is well suited for geographic analysis scenarios, but it is not a full GIS suite; however, its extensible programming model allows it to act as a potential basis for one.

Terrascale consists of the following software components:

* TerrascaleDB, a cloud-native distributed storage and query engine
* The Tile Delivery Network (TDN), a CDN for raster image tiles
* Terrascale's Visualizer, a high-performance in-browser 2D rendering engine and database client
* Terrascale Projects, a real-time collaborative, version-controlled studio for scientific analysis
* The Terrascale API, which provides frontend and backend programability for the above
* Terrascale, an in-browser webapp which provides a unified experience for all the above

Although the initial build of Terrascale will be optimized for cloud computing scenarios, we plan to design up front to make it feasible to deploy Terrascale on the edge as a ruggedized portable box containing a small cluster of compute/storage nodes running Terrascale. For example, we abstract away cloud-native storage systems and compute orchestration APIs to make it simpler to build a local clustering system, and we have designed Terrascale to work with a very small number of compute nodes. Fully lighting up this scenario would also involve the introduction of a new synchronization engine, allowing a user to select subsets of their cloud dataset to take with them on the go; this is out of scope for this document.

In this document, we will build from the bottom up, starting from Terrascale's cloud-native data management and tile rendering architecture, then moving onto client-side experiences such as the visualization engine and extensibility points, and finally onto the Projects experience hosted on Terrascale's website.

## Cloud Provisioning

Terrascale is a multi-tenant service through which customers manage single-tenant TerrascaleDB instances running in a third-party cloud such as AWS, Azure or GCP. In this way, TerrascaleDB clusters are similar to Snowflake's virtual warehouse concept. This gives customers fine-grained control over the compute-vs-cost tradeoff, and isolates resource-intensive analytical queries to the customer's private compute cluster. This also provides strong isolation of customer data for security-sensitive scenarios than a multi-tenant compute cloud could provide.

## Database Workloads

TerrascaleDB is a hybrid transactional/analytical database with support for high-volume short-lived transactional queries in tandem with low-volume resource-intensive analytical (cross-tabulation) queries. Queries can execute on a finite point in time, or update continually over real-time streaming data. Users can independently scale compute and storage resources as a means to control the cost-performance tradeoff of their queries.

Kinds of queries we expect and optimize for include...

**Point reads and writes**: Workloads where users build their datasets from operational data such as sensor data ingestion or logging human interactions with software, will be characterized by point writes, potentially at a high rate with high parallelism, with each write inserting a single row. From Terrascale's point of view, these workloads may be write-only (when users host their operations in a separate system and stream updates to Terrascale, e.g. using a mechanism like Postgres's change data capture), or they may be a read-write-mix if users directly host their operations in Terrascale. In general, the point reads and writes generated by this aspect of a customer workload will each be confined to a single table partition.

**Time- and space-windowed queries**: As data streams into a Terrascale instance, we expect to see queries over small space and time ranges as users export their data for human visualization or programmatic observability (monitoring and alerts). We usually expect these small windowing queries to run periodically at a low rate, but anticipate that sometimes these views may be "slashdotted" and thus see a high rate of read-only traffic. These queries may run finitely or use incremental streaming, and each query will usually cross only a small number of table partitions.

**Bulk import**: Customers using Terrascale for offline analytics may bulk-import data from other sources, either as a one-time event to populate a cluster or as part of a nightly ETL database dump. This kind of workload is characterized by just one or two write transactions each inserting many, many rows, crossing all partitions of a cluster. It is unusual for customers to run other kinds of queries while this is ongoing; for example, an ETL customer will likely load all data into Terrascale before starting any new analytical queries. As such, it is okay for a bulk import to cause a storm of traffic that doesn't settle until the import has completed. Bulk import operations need to be restartable, whether through a checkpoint/restart protocol or idempotency.

**Analysis**: Users are expected to use their datasets, whether built operationally or via obtained via a bulk import, for large cross-tabulating analysis queries featuring complex filters, grouping, aggregations, cross-table joins, and spatial joins. Although users are expected to run few of theses kinds of queries at a time, each may necessitate a large amount of compute, storage and network traffic. Additionally, analytical queries may either run in a finite point-in-time or as incremental realtime streaming; the latter is useful for scenarios like anomaly detection and alerting.

**Complex updates**: As part of cleaning and maintaining large and complex datasets, we expect users will occasionally need to issue one-off 'corrections' to their datasets. These are characterized by large, complex transactions which may read many rows, do some sort of computation, and push many updates. Data cleaning transactions and table schema changes are examples of this kind of update. These large transactions are challenging to handle well, since they may cross many table partitions and need to have full serializable semantics; however, because they are low volume and only happen occassionally, we don't optimize for these. We simply ensure these produce the correct results without causing excessive downtime for real-time and operational workloads.

**Bulk export**: Customers may wish to do large, point-in-time bulk reads to export data from Terrascale as part of a wider ETL process that copies or backs up Terrascale data to a separate system. Like bulk import, exports need to provide consistency and restartability, whether the latter is achieved through idempotency or a checkpoint/restart protocol.

## Storage Model

TerrascaleDB's cloud native storage model features tiered abstractions over underlying storage, allowing efficient utilization of a variety of cloud-native and local unstructured data storage systems. Although we initially intend TerrascaleDB to run on the cloud and use cloud-native object and block stores, we anticipate a future need to run TerrascaleDB locally, e.g. as a small cluster of nodes inside a ruggedized box. As such, the TerrascaleDB storage model is flexible, abstract and software-defined.

---

TODO FIXME per the notes below. I think we *just* abstract over cloud storage and rely on those for replication and/or erasure coding. In the future, if we ever support a ruggedized box kind of scenario where we run on our own hardware, then our internal block/object store abstractions over local file systems have to include a replication and/or erasure coding scheme locally within the abstraction.

We still need our own checksums, and encryption needs to be on our radar.

I need to double-check, but I think I was also wrong to assume it's possible to continually append to an object; I suspect an S3 object is truly one and done. That means we need to support a destaging process by which an incrementally built log in a block store can be destaged to an object once it's "done" being built. This is how the LSM layer would handle checkpoints, for example.

I also don't know exactly how we'd want to do key-value stores for things like database catalogs. Probably these are just overwriting objects in an object store using some kind of versioning scheme.

Handling eventual consistency in S3 is a problem.

Managing S3 buckets / blob containers / etc is another problem.

We may need a separate metadata store to track what exists, what needs cleaning up, and what we need to not forget exists in an eventually consistent store, but that adds another degree of complexity I'd like to avoid. I suppose this is why Snowflake has so many blogs about FDB ... it's probably acting as this bookkeeping system.

---

TerrascaleDB's storage plane consists of the following layer stack:

* A bottommost **store layer** abstracts over block and object stores
* On top of that, a **protection layer** provides replication, erasure coding, checksums and encryption
* At the top is the **abstraction layer** which implements append-only logs and key-value stores

TODO so uh, how exactly does this work anyways? For the store layer, we need a URI scheme, we need abstract APIs over a block or an object storage system. For the protection layer, the main question is whether we can provide protection semantics at the object and/or block level, or if we need to integrate protection into higher layer abstractions. And, for the abstraction layer, we need to detail the abstractions we provide. For example, the append-only logs can either run on contiguous blocks in a block store as as a single incrementally built object in an object store ... or, do we really have to create objects once only on some providers? It seems I have some research to do here.

One thing worth noting is that I'm thinking about this in a very XStore way; the fact is, cloud blocks and objects are already replicated for us, so questions about protection really come down to situations where we run on local clusters. So we likely can get away with abstracting over block stores and object stores, and only thinking about replication and erasure coding for on-prem block/object storage.

I think, if object stores are truly one-and-done, then a simple strategy might be to implement append-only logs on block storage only, and support idempotent archival of a log or section of a log to an object. The LSM engine can then incrementally checkpoint pages to a log and, when done, lazily destage that log section to an object and collect the underlying extents.

## Indexing Structures

TerrascaleDB indexes data using a log-structured merge strategy which combines row-oriented and columnar inverted indexing. New rows are logged to a dedicated row log, and are indexed in a row index and a column index, each of which is log-structured merge over B+ trees which bottom out to pointers into the row log. The row log is managed using a copy-forward garbage collection scheme. Some aggregate information is stored inside these columnar LSM Trees to accelerate SQL aggregate queries.

This design mixes ideas found in Apache Druid and WiscKey.

### The Row Index

TerrascaleDB's row index is log-structured merge over B+ trees with rows stored out of the tree, as suggested in the WiscKey paper. Because row payloads are not stored in the tree and keys are generally assumed to be small, the row index itself is not large relative to the size of the row array, making it possible to have only a few levels of the tree and simple merge policies, without fear of ballooning write amplification.

### The Column Index

TerrascaleDB's column index uses an inverted indexing strategy inspired by Apache Druid, which is in turn inspired by document search databases such as Apache Lucene.

A traditional column store stores data in one array per column, where the $i^{th}$ entry in a column array is the value for that column for the $i^{th}$ row. In this scheme, the $i^{th}$ row can be read in its entirey by reading the $i^{th}$ entry of each column array and concatenating the values to obtain the original row. This tends to work well for queries which run aggregate functions like min, max, and average over a single column, since all the values that need to be calculated on appear in a single file, with no unrelated data in the same file. In addition, the simple structure of these files makes them amenable to simple but highly effective compression schemes, which further saves I/O bandwidth in a large 

In a Druid-like inverted column store, columns are instead indexed by their values. This means each entry is a key-value pair mapping (the value that appears for this column in one or more rows of the table) to (indexes of rows which have this value for this column). For example, if an $Addresses$ table had a column $City$, and the $i^{th}$ row had the value $Bellevue$ for $City$, then the $City$ index of the $Addresses$ table would map the value $Bellevue$ to a set of row IDs including $i$, since row $i$ has $City=Bellevue$. This scheme is particularly effective when a column value appears in many rows. The set of row IDs for a given column is usually stored in a compressed bitmap structure such as Roaring Bitmaps.

In TerrascaleDB, the column index for a given column is an LSM Tree which maps column values to a set of matching rows. For values that only appear in a few rows, the row pointers are stored directly in the tree; for larger row sets, the rows are stored in an external roaring bitmap. For each row set, a result count is stored directly in the tree. 

### Write-Ahead, Checkpoint and Merge



### Garbage Collection



### Queries and Access Paths



## Aggregate Analysis

TODO Anything you can do on unsorted columnar data, you can also do on an inverted index. When you come across a column value in an inverted index, you repeat whatever you would have done with your columnar forward index as many times as there are matching bits in the bitset.

More concrete examples: min just gets the lowest matching column value that has nonzero bits, count distinct and sum just look at number of instances of each column value, and mean is just sum divided by count. Some functions like ranks and percentiles need larger scans and may require more work, and/or approximation algorithms if we can find them (think like the quick select median of medians).

## Transactions and Concurrency Control

TODO this is an LSN-ordered publish log for write-only (WO) transactions, snapshot isolation for read-only (RO) transactions at any scale (point, window, analytical), and a predicate locking scheme for read-write (RW) transactions for the complex update scenario. WO transactions are written to one of the active write-ahead logs with a single record that describes and commits the transactions, whereas RW transactions commit incrementally, with the final commit piggybacked on the last incremental write. Recovery replays committed transactions and ignores uncommited ones; since this is an LSM, there is only a log, so there is nothing to undo.

The problem that needs to be solved here is how to rectify an ahead-of-time LSN ordering scheme with long running RW transactions with unpredictable predicate locking. If the predicates can be found ahead of time, we can ensure serializability easily, but if RW transactions can incrementally lock more and more predicates, it's not clear how to let that happen without locking the whole WO publish queue. An RW transaction can

But that's fine, I think we document two schemes. If it's possible to predicate lock for an RW transaction ahead of time, then the RW transaction simply acquires all predicate locks, waits for all preceding RW or WO transaction, then executes on its own isolated snapshot view of the world. If for some reason that is not feasible for some queries, those must lock the entire WO queue, which sucks but at least we can do some background buffering so when RW finally publishes, all WOs that were waiting on it show up instantly as well.

## Query Path Selection and Optimization

## Distributed SQL Queries

TODO this is raft for distributed commit of transactions. A good question is who plans and optimizes the query. It could be a random node in the network, or it could be the raft leader (if that wouldn't be a bottleneck, at least the leader is always up to date, right?) or it could be a multi-tenant cloud orchestration service sitting in front of the cluster (but I'd rather do everything in the cluster if possible, since we want to keep open the possibility of shipping a small cluster of nodes in a box).

## Geospatial

TODO this is columnar range breakdown to find matching items within bounding boxes, then analytical collision detection and other spatial algorithms to finish joins and whatnot

## Incremental Queries and Real-Time Streaming

TODO this is mostly a division into cases, what kinds of things the language supports and how we update these over time.

## Imperative Language Support

TODO I want this to essentially be a way of executing queries, without having to build/parse SQL strings, and help cast them to the types that users want to run calculations over, like numpy arrays or that nd thing that I remember seeing geo people really excited about

## Tile Delivery Network

TODO is this third party CDN? Nodes serving data from inside your network? Just putting tiles into objects and serving them over cloud URLs? Any processing that has to be done, you can do inside your processing cluster, right?

## Tile Processing

TODO obviously this can't be SQL. Should it be shaders? Who executes the shaders?

## Embeddable, Programmable Visualizer

TODO this is a 2D rendering engine for tile and vector data that runs in a native app using C/Vulkan or on the web with Emscripten/WebGPU. 

## Projects and Version Control

TODO define some kind of project object model, introduce CRDTs for common operations, describe a database scheme for tracking version history

## Terrascale.io

TODO first party hosting for all this stuff. Billing and stuff.
